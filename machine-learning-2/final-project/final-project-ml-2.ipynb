{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":269359,"sourceType":"datasetVersion","datasetId":111880}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Classification-CNN Model\n\n### Anggota Kelompok\n- Rizky Cahyono Putra\n- Raffa Arvel Nafi'Nadindra\n- M. Irfansyah\n- Galang Fachrezy\n- Syaifan Nur Iwawan","metadata":{}},{"cell_type":"markdown","source":"Hasil Notebook ini adalah **Klasifikasi Gambar** dari dataset **Intel Image Classification dataset** terbagi menjadi 6 kategori:\n\n1. Buildings  \n2. Forest  \n3. Glacier  \n4. Mountain  \n5. Sea  \n6. Street  ","metadata":{}},{"cell_type":"markdown","source":"## Import liberary\n\nMenimport beberapa library","metadata":{}},{"cell_type":"code","source":"# Basic imports\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sn; sn.set(font_scale=1.2)\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tqdm import tqdm\n\n# Classes\nclass_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\nclass_names_label = {name: i for i, name in enumerate(class_names)}\nnb_classes = len(class_names)\n\n# Image size\nIMAGE_SIZE = (150, 150)\n\n# Training epochs\nEPOCHS = 20\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fungsi untuk Memuat dan Merapikan Data\n\nKita membuat sebuah fungsi `load_data()` untuk melakukan semua tugas persiapan data secara otomatis. Fungsi ini akan:\n- Mengakses folder data training dan data testing.\n- Membaca setiap gambar satu per satu.\n- Mengubah ukuran semua gambar menjadi **150x150 piksel** agar seragam.\n- Mengubah format warna dari BGR ke RGB agar sesuai standar.\n- Memberi label angka pada setiap gambar (misalnya, 'forest' menjadi 1, 'sea' menjadi 4).\n- **Normalisasi**, yaitu mengubah nilai piksel dari rentang 0-255 menjadi 0-1 agar model lebih mudah belajar.","metadata":{}},{"cell_type":"code","source":"def load_data():\n    datasets = [\n        '../input/intel-image-classification/seg_train/seg_train',\n        '../input/intel-image-classification/seg_test/seg_test'\n    ]\n    \n    output = []\n    \n    for dataset in datasets:\n        images = []\n        labels = []\n        print(\"Loading {}\".format(dataset))\n        \n        for folder in os.listdir(dataset):\n            label = class_names_label[folder]\n            \n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                img_path = os.path.join(dataset, folder, file)\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, IMAGE_SIZE)\n                images.append(image)\n                labels.append(label)\n                \n        images = np.array(images, dtype='float32') / 255.0  # normalize\n        labels = np.array(labels, dtype='int32')\n        output.append((images, labels))\n    \n    return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the dataset \n\nSekarang kita panggil fungsi `load_data()` yang sudah dibuat. Setelah data berhasil dimuat, kita **mengacak (shuffle)** urutan data training. Tujuannya agar model tidak menghafal urutan data dan bisa belajar lebih baik.","metadata":{}},{"cell_type":"code","source":"# Load train and test data\n(train_images, train_labels), (test_images, test_labels) = load_data()\n\n# Shuffle train data\ntrain_images, train_labels = shuffle(train_images, train_labels, random_state=25)\n\nprint(\"Number of training examples:\", train_labels.shape[0])\nprint(\"Number of testing examples:\", test_labels.shape[0])\nprint(\"Each image size:\", IMAGE_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize random images per class","metadata":{}},{"cell_type":"code","source":"def display_random_image_per_class(class_names, images, labels):\n    plt.figure(figsize=(12,6))\n    \n    for i, class_name in enumerate(class_names):\n        class_indices = np.where(labels == i)[0]\n        index = np.random.choice(class_indices)\n        plt.subplot(2, 3, i+1)\n        plt.imshow(images[index])\n        plt.title(class_name)\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_random_image_per_class(class_names, train_images, train_labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Membangun Arsitektur Model CNN \n\nDi sini, kita akan merancang arsitektur atau \"cetak biru\" dari model kita dari awal. Bayangkan kita sedang membangun gedung bertingkat:\n- **`Conv2D`**: Ini adalah \"mata\" model kita. Layer ini bertugas mendeteksi pola-pola dasar pada gambar seperti garis, sudut, dan tekstur. Semakin dalam, polanya semakin kompleks.\n- **`MaxPooling2D`**: Layer ini berfungsi untuk meringkas informasi, mengambil bagian terpenting dari pola yang ditemukan `Conv2D` dan mengurangi ukuran data.\n- **`Flatten`**: Mengubah data yang tadinya berbentuk seperti gambar (2D) menjadi satu baris panjang (1D), agar bisa diproses oleh \"otak\" model.\n- **`Dense`**: Ini adalah \"otak\" model. Layer ini bertugas mengambil keputusan berdasarkan informasi yang sudah diringkas.\n- **`Dropout`**: Teknik untuk mencegah model menjadi \"terlalu pintar\" atau *overfitting* (hanya hafal data training tapi tidak bisa menebak data baru).\n- **`softmax`**: Layer terakhir yang memberikan hasil akhir berupa probabilitas untuk setiap kelas (misalnya, 70% gambar ini adalah 'laut', 20% 'gletser', dst).\n\nSetelah arsitektur siap, kita **compile** model dengan menentukan `optimizer` (cara model belajar), `loss` (cara mengukur kesalahan), dan `metrics` (metrik keberhasilan, yaitu akurasi).","metadata":{}},{"cell_type":"code","source":"model = models.Sequential([\n    layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    \n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    \n    layers.Conv2D(128, (3,3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    \n    layers.Conv2D(256, (3,3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    \n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(nb_classes, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer Learning\n\nSelain membuat dari nol, ada cara yang lebih cerdas yaitu **Transfer Learning**. Kita tidak membangun dari awal, tapi memakai model canggih bernama **EfficientNetB0** yang sudah dilatih oleh Google pada jutaan gambar.\n\nCaranya:\n1.  **Muat Model EfficientNetB0**: Kita ambil seluruh arsitekturnya, tapi buang layer paling atas (layer klasifikasinya).\n2.  **Bekukan (Freeze)**: Kita kunci semua layer yang sudah pintar ini agar tidak berubah saat dilatih ulang.\n3.  **Tambahkan \"Kepala\" Baru**: Kita hanya menambahkan beberapa layer `Dense` baru di bagian atas yang akan kita latih khusus untuk mengenali 6 kelas dataset kita.\n\nMetode ini biasanya jauh lebih cepat dan memberikan akurasi yang lebih tinggi.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n\n\nbase_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(150,150,3))\nbase_model.trainable = False  # Freeze pretrained layers\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(nb_classes, activation='softmax')(x)\n\nefficient_model = Model(inputs=base_model.input, outputs=predictions)\n\nefficient_model.compile(optimizer=Adam(1e-4),\n                        loss='sparse_categorical_crossentropy',\n                        metrics=['accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Model\n\n- **`epochs=20`**: Artinya, model akan \"mempelajari\" seluruh data training sebanyak 20 kali putaran.\n- **`validation_data`**: Selama belajar, model akan sesekali \"mengerjakan ujian\" menggunakan data validasi untuk mengukur sejauh mana kemampuannya pada data yang belum pernah ia lihat.\n- **`ReduceLROnPlateau`**: Ini seperti guru yang sabar. Jika nilai model tidak membaik setelah beberapa putaran, \"guru\" ini akan menurunkan kecepatan belajar agar model bisa belajar lebih teliti.","metadata":{}},{"cell_type":"code","source":"# Reduce learning rate if val_loss plateaus\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n\n# Fit the model\nhistory = model.fit(\n    train_images, train_labels,\n    epochs=EPOCHS,\n    validation_data=(test_images, test_labels),\n    batch_size=32,\n    callbacks=[reduce_lr]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Menganalisis Proses Belajar: Grafik Akurasi & Loss\n\nSetelah training selesai, mari kita lihat \"rapor\" belajarnya. Kita akan membuat grafik untuk melihat:\n- **Grafik Akurasi**: Apakah akurasi model terus meningkat, baik pada data training maupun validasi?\n- **Grafik Loss (Kesalahan)**: Apakah tingkat kesalahan model terus menurun?\n\nGrafik ini sangat penting untuk mendeteksi apakah model kita *overfitting* atau tidak.","metadata":{}},{"cell_type":"code","source":"def plot_accuracy_loss(history):\n    plt.figure(figsize=(12,5))\n    \n    # Accuracy\n    plt.subplot(1,2,1)\n    plt.plot(history.history['accuracy'], 'bo--', label=\"train_accuracy\")\n    plt.plot(history.history['val_accuracy'], 'ro--', label=\"val_accuracy\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    \n    # Loss\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'], 'bo--', label=\"train_loss\")\n    plt.plot(history.history['val_loss'], 'ro--', label=\"val_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    \n    plt.show()\n\nplot_accuracy_loss(history)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Mengevaluasi Hasil Akhir: Confusion Matrix & Laporan Klasifikasi\n","metadata":{}},{"cell_type":"code","source":"# Predict test images\ny_pred = model.predict(test_images)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Confusion matrix\ncm = confusion_matrix(test_labels, y_pred_classes)\nplt.figure(figsize=(8,6))\nsn.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification report\nprint(classification_report(test_labels, y_pred_classes, target_names=class_names))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Investigasi Kesalahan: Melihat Gambar yang Salah Tebak\n\nTidak ada model yang sempurna. Mari kita lihat beberapa contoh gambar di mana model kita salah menebak. Ini membantu kita memahami kelemahan model dan mencari cara untuk memperbaikinya di kemudian hari.","metadata":{}},{"cell_type":"code","source":"def display_mislabeled(class_names, images, true_labels, pred_labels, n=25):\n    mislabeled_indices = np.where(true_labels != pred_labels)[0]\n    plt.figure(figsize=(12,12))\n    for i in range(min(n, len(mislabeled_indices))):\n        idx = mislabeled_indices[i]\n        plt.subplot(5,5,i+1)\n        plt.imshow(images[idx])\n        plt.title(f\"True: {class_names[true_labels[idx]]}\\nPred: {class_names[pred_labels[idx]]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndisplay_mislabeled(class_names, test_images, test_labels, y_pred_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}